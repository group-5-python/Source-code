# -*- coding: utf-8 -*-
"""mazenogilzene.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpd31rMJvHdWxHxrFGs0MSB-n0XX5Ldj
"""


# Data manipulation and analysis
import pandas as pd
import numpy as np

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
from scipy.stats import uniform
!pip install streamlit
import streamlit as st


from google.colab import drive

drive.mount('/content/gdrive')

#loading the dataset

vehicles_df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ITT316/FinalProject/vehicles.csv')

# Identifying missing values
missing_values = vehicles_df.isnull().mean().sort_values(ascending=False) * 100
print(missing_values)

# Drop columns with more than 50% missing values
threshold = 50
columns_to_drop = missing_values[missing_values > threshold].index
vehicles_df = vehicles_df.drop(columns=columns_to_drop)

# Display the remaining columns after dropping
print("Remaining columns after dropping those with > 50% missing values:")
print(vehicles_df.columns)

# Checking the data types of the remaining columns
print("Data types of remaining columns:")
print(vehicles_df.dtypes)

# Determine categorical and numerical columns
categorical_columns = vehicles_df.select_dtypes(include=['object']).columns
numerical_columns = vehicles_df.select_dtypes(include=[np.number]).columns

# Fill missing values for categorical columns with the mode
for col in categorical_columns:
    mode_value = vehicles_df[col].mode()[0]
    vehicles_df[col] = vehicles_df[col].fillna(mode_value)

# Fill missing values for numerical columns with the median
for col in numerical_columns:
    median_value = vehicles_df[col].median()
    vehicles_df[col] = vehicles_df[col].fillna(median_value)

# Verify if there are any remaining missing values
print("Remaining missing values after imputation:")
print(vehicles_df.isnull().sum())

from sklearn.preprocessing import LabelEncoder

# Apply Label Encoding to all categorical columns
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    vehicles_df[col] = le.fit_transform(vehicles_df[col])
    label_encoders[col] = le  # Save the encoder for inverse transformation if needed

# Display the first few rows of the encoded data
print("Data after Label Encoding:")
print(vehicles_df.head())

# Check data types to ensure all are numeric
print("Data types after encoding:")
print(vehicles_df.dtypes)

# Display the first few rows to verify the encoded values
print("First few rows of the encoded dataset:")
print(vehicles_df.head())

# Final check for missing values
print("Any remaining missing values:")
print(vehicles_df.isnull().sum())

# Summary statistics for numerical columns
print("Summary statistics for numerical columns:")
print(vehicles_df.describe())

# Distribution of price
plt.figure(figsize=(10, 6))
sns.histplot(vehicles_df['price'], bins=50, kde=True)
plt.title('Distribution of Price')
plt.show()

# Distribution of year
plt.figure(figsize=(10, 6))
sns.histplot(vehicles_df['year'], bins=30, kde=True)
plt.title('Distribution of Year')
plt.show()

# Distribution of odometer
plt.figure(figsize=(10, 6))
sns.histplot(vehicles_df['odometer'], bins=50, kde=True)
plt.title('Distribution of Odometer')
plt.show()

# Correlation matrix
corr_matrix = vehicles_df.corr()

# Heatmap of correlations
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# Boxplot for price
plt.figure(figsize=(10, 6))
sns.boxplot(x=vehicles_df['price'])
plt.title('Boxplot of Price')
plt.show()

# Boxplot for odometer
plt.figure(figsize=(10, 6))
sns.boxplot(x=vehicles_df['odometer'])
plt.title('Boxplot of Odometer')
plt.show()

# removing outliers using IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

# Remove outliers for 'price' and 'odometer'
vehicles_df = remove_outliers(vehicles_df, 'price')
vehicles_df = remove_outliers(vehicles_df, 'odometer')

# Re-check distributions after outlier removal
print("Distribution of Price after outlier removal:")
sns.histplot(vehicles_df['price'], bins=50, kde=True)
plt.show()

print("Distribution of Odometer after outlier removal:")
sns.histplot(vehicles_df['odometer'], bins=50, kde=True)
plt.show()

#FEATURE ENGINEERING
# Removing non-informative columns
non_informative_columns = ['id', 'url', 'region_url', 'image_url', 'VIN']
vehicles_df = vehicles_df.drop(columns=non_informative_columns)

# Verify remaining columns
print("Remaining columns after dropping non-informative ones:")
print(vehicles_df.columns)

#Given the skewness of the price and odometer, we will transform the variable

#FEATURE TRANSFORMATION
# Apply log transformation to 'price' and 'odometer'
vehicles_df['log_price'] = np.log1p(vehicles_df['price'])  # log1p to handle zero values
vehicles_df['log_odometer'] = np.log1p(vehicles_df['odometer'])

# Verify the transformation
print("First few rows after log transformation:")
print(vehicles_df[['log_price', 'log_odometer']].head())

#FEATURE EXTRACTION
# Create a new feature: Age of the car
vehicles_df['car_age'] = 2024 - vehicles_df['year']

# Verify the new feature
print("First few rows with the new 'car_age' feature:")
print(vehicles_df[['year', 'car_age']].head())

#fEATURE SELECTION
# Selecting features for the model
selected_features = ['log_price', 'log_odometer', 'car_age', 'manufacturer', 'model', 'condition', 'cylinders',
                     'fuel', 'title_status', 'transmission', 'drive', 'type', 'paint_color', 'state']

# Subset the dataset to only include these features
vehicles_df = vehicles_df[selected_features]

# Verify the final set of features
print("Final set of features for model training:")
print(vehicles_df.head())

from sklearn.model_selection import train_test_split

# Defining the target variable and features
X = vehicles_df.drop(columns=['log_price'])
y = vehicles_df['log_price']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Verify the split
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

# Initialize and train the Linear Regression model
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

# Predict on the test set
y_pred_linear = linear_reg.predict(X_test)

# Calculate and display metrics
mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = mse_linear ** 0.5
mae_linear = mean_absolute_error(y_test, y_pred_linear)
r2_linear = r2_score(y_test, y_pred_linear)

print(f"Linear Regression Metrics:")
print(f" - Mean Squared Error (MSE): {mse_linear}")
print(f" - Root Mean Squared Error (RMSE): {rmse_linear}")
print(f" - Mean Absolute Error (MAE): {mae_linear}")
print(f" - R-squared (R²): {r2_linear}")

# Initialize and train the Random Forest model
rf_reg = RandomForestRegressor(random_state=42)
rf_reg.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_reg.predict(X_test)

# Calculate and display metrics
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = mse_rf ** 0.5
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest Regressor Metrics:")
print(f" - Mean Squared Error (MSE): {mse_rf}")
print(f" - Root Mean Squared Error (RMSE): {rmse_rf}")
print(f" - Mean Absolute Error (MAE): {mae_rf}")
print(f" - R-squared (R²): {r2_rf}")

from sklearn.ensemble import GradientBoostingRegressor

# Initialize and train the Gradient Boosting model
gb_reg = GradientBoostingRegressor(random_state=42)
gb_reg.fit(X_train, y_train)

# Predict on the test set
y_pred_gb = gb_reg.predict(X_test)

# Calculate and display metrics
mse_gb = mean_squared_error(y_test, y_pred_gb)
rmse_gb = mse_gb ** 0.5
mae_gb = mean_absolute_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

print(f"Gradient Boosting Regressor Metrics:")
print(f" - Mean Squared Error (MSE): {mse_gb}")
print(f" - Root Mean Squared Error (RMSE): {rmse_gb}")
print(f" - Mean Absolute Error (MAE): {mae_gb}")
print(f" - R-squared (R²): {r2_gb}")

# Model Initialization
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest Regressor": RandomForestRegressor(random_state=42),
    "Gradient Boosting Regressor": GradientBoostingRegressor(random_state=42)
}

# Model Training and Evaluation
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = mse ** 0.5
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{name} Metrics:")
    print(f" - Mean Squared Error (MSE): {mse}")
    print(f" - Root Mean Squared Error (RMSE): {rmse}")
    print(f" - Mean Absolute Error (MAE): {mae}")
    print(f" - R-squared (R²): {r2}")

# Hyperparameter Tuning Example for Random Forest
param_dist = {
    'n_estimators': [50, 100, 200],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}


rf = RandomForestRegressor(random_state=42)
grid_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)
grid_search.fit(X_train, y_train)
best_rf = grid_search.best_estimator_

# Print best parameters
print(f"Best parameters for Random Forest Regressor: {grid_search.best_params_}")

# Model Training and Evaluation for the tuned Random Forest
y_pred_best_rf = best_rf.predict(X_test)
mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)
rmse_best_rf = mse_best_rf ** 0.5
mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)
r2_best_rf = r2_score(y_test, y_pred_best_rf)

print(f"Tuned Random Forest Regressor Metrics:")
print(f" - Mean Squared Error (MSE): {mse_best_rf}")
print(f" - Root Mean Squared Error (RMSE): {rmse_best_rf}")
print(f" - Mean Absolute Error (MAE): {mae_best_rf}")
print(f" - R-squared (R²): {r2_best_rf}")

# Streamlit GUI
def main():
    st.title('Car Price Prediction')

    st.sidebar.header('User Input Parameters')

    # Input fields
    make = st.sidebar.selectbox('Make', vehicles_df['manufacturer'].unique())
    model = st.sidebar.selectbox('Model', vehicles_df['model'].unique())
    year = st.sidebar.slider('Year', min_value=2000, max_value=2024, value=2020)
    mileage = st.sidebar.slider('Mileage (in miles)', min_value=0, max_value=300000, value=50000)
    condition = st.sidebar.selectbox('Condition', vehicles_df['condition'].unique())

    # Prepare input data
    input_data = pd.DataFrame({
        'manufacturer': [make],
        'model': [model],
        'year': [year],
        'odometer': [mileage],
        'condition': [condition]
    })
    input_data['car_age'] = 2024 - input_data['year']
    input_data['log_odometer'] = np.log1p(input_data['odometer'])
    input_data = input_data[['log_odometer', 'car_age', 'manufacturer', 'model', 'condition']]

    # Predict using the best model
    if st.sidebar.button('Predict'):
        prediction = best_rf.predict(input_data)
        st.write(f"Predicted Price: ${np.expm1(prediction[0]):,.2f}")

if __name__ == "__main__":
    main()

streamlit run mazenogilzene (1).py
